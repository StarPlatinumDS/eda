{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ],
   "id": "fa38574a31fe7e7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load data",
   "id": "3ea61efa7028769"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df = pd.read_csv('Housing.csv')\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.info()",
   "id": "7361af486191b539",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Encode categorical data\n",
    "def encode_yes_no(value):\n",
    "    return 1 if value == 'yes' else 0 if value == 'no' else value # in columns w/ yes or no we swap to 1 & 0\n",
    "\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    if set(df[col].unique()) <= {'yes', 'no'}:\n",
    "        df[col] = df[col].apply(encode_yes_no)\n",
    "\n",
    "df['furnishingstatus'].replace(to_replace=['furnished', 'semi-furnished', 'unfurnished'], value=[2, 1, 0], inplace=True) #in this column we swap to 2, 1, 0"
   ],
   "id": "66d26c91157b9b37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data preprocessing",
   "id": "28bce0e0415adaea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Check for missing values\n",
    "df.isnull().sum()"
   ],
   "id": "e3efea9934af1eef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Correlation Analysis",
   "id": "2097e5790e45ec81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Pearson's corr\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Pearson Feature Correlation')\n",
    "plt.show()"
   ],
   "id": "78af21604e1b3590",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Kendall\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df.corr(method='kendall'), annot=True, cmap='coolwarm')\n",
    "plt.title('Kendall Feature Correlation')\n",
    "plt.show()"
   ],
   "id": "e4abf8ab69373f41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Spearman\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df.corr(method='spearman'), annot=True, cmap='coolwarm')\n",
    "plt.title('Spearman Feature Correlation')\n",
    "plt.show()"
   ],
   "id": "79263477113e4631",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Masking redundant values that do not have high correlation\n",
    "corr = df.corr(method='kendall')\n",
    "threshold = 0.19  # Adjust as needed\n",
    "\n",
    "# Mask values below the threshold\n",
    "mask = np.abs(corr) < threshold\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    mask=mask,\n",
    "    annot_kws={'size': 8},  # Smaller annotations\n",
    "    fmt=\".2f\"\n",
    ")\n",
    "plt.title(f'Masked Correlation Heatmap (abs(corr) â‰¥ {threshold})')\n",
    "plt.show()"
   ],
   "id": "37faabb53cc68f3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### I'm choosing kendall's correlation because it's more suitable for non-linear data, the dataset is relatively small and (as I think) it better shows correlation to the target variable in this specific dataset",
   "id": "8463dc2a3fbbbeca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Now let's split data into train & test, while also removing features that do not have high correlation with our target (Price)",
   "id": "8d352eaaa7d67c73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = df.drop(['basement', 'hotwaterheating'], axis=1)",
   "id": "e5a45f50a53e37e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Split into train & test\n",
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f'Training Features shape: {X_train.shape}, Test Features Shape: {X_test.shape}')"
   ],
   "id": "92ce8c9073ddfdb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# XGB Model",
   "id": "16ec084024e0ddb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "xgb = XGBRegressor(random_state=42)\n",
    "xgb.fit(X_train, y_train)"
   ],
   "id": "1e1e5849ca872a8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Predictions\n",
    "y_pred = xgb.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Base XGB Model:\\nRMSE: {rmse: .2f}\\nR2 Score: {r2: .2f}')"
   ],
   "id": "e22d62ac2d76ef55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Fine tune w/ GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "print('Optimization initialized.')\n",
    "grid_search = GridSearchCV(estimator=XGBRegressor(random_state=42), param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, verbose=1, n_jobs=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Here's what we found:\", grid_search.best_params_)"
   ],
   "id": "8a3fb3edae8ca89e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Model using optimized parameters\n",
    "better_xgb = grid_search.best_estimator_\n",
    "y_pred_better = better_xgb.predict(X_test)\n",
    "rmse_better = np.sqrt(mean_squared_error(y_test, y_pred_better))\n",
    "r2_better = r2_score(y_test, y_pred_better)\n",
    "\n",
    "print(f\"\\nOptimized XGBoost Model:\\nRMSE: {rmse_better:.2f}\\nR2: {r2_better:.2f}\")\n"
   ],
   "id": "2fabfd6e0040a6d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ....",
   "id": "228a5e73dff65636"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "39a6aa9df9e3b09e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e7b2eb5827b5a908"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b1fb08964bd2227f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualize",
   "id": "35cdc7334835d120"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(y_test, y_pred_better, alpha=0.6, color='orange')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\n",
    "plt.title('Real Price vs. Predicted')\n",
    "plt.xlabel('Real Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.show()"
   ],
   "id": "5fcd5e7583e24bb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7c0714be8f0be0d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Fine tune w/ RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform  # For parameter distributions\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 300),  # Random integers in range\n",
    "    'learning_rate': uniform(0.01, 0.3),  # Random floats in range\n",
    "    'max_depth': randint(3, 10),  # 3 to 9\n",
    "    'subsample': uniform(0.5, 0.5)  # 0.5 to 1.0\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearch\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # Number of random combinations to try\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"Best params:\", random_search.best_params_)"
   ],
   "id": "3b93d35e38583943",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Model using optimized parameters\n",
    "rand_better_xgb = random_search.best_estimator_\n",
    "y_pred_rand_better = better_xgb.predict(X_test)\n",
    "rand_rmse_better = np.sqrt(mean_squared_error(y_test, y_pred_rand_better))\n",
    "rand_r2_better = r2_score(y_test, y_pred_rand_better)\n",
    "\n",
    "print(f\"\\nOptimized XGBoost Model:\\nRMSE: {rand_rmse_better:.2f}\\nR2: {rand_r2_better:.2f}\")"
   ],
   "id": "509a841291dd7bae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(y_test, y_pred_rand_better, alpha=0.6, color='orange')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\n",
    "plt.title('Real Price vs. Predicted')\n",
    "plt.xlabel('Real Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.show()"
   ],
   "id": "7f0968d24be910eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e8c74af32edc83b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "75bf48bb76a1ce5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "12d756b4aa4c318e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "93976a3e40ffc080"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let's try optimizing with Optuna",
   "id": "2f95efe635e760b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Code is the same up to creating X and y variables, we're going to try log transform on 'price' in order to minimize error\n",
    "\n",
    "\n",
    "#Split into train & test\n",
    "X_o = df.drop('price', axis=1)\n",
    "y_o = np.log1p(df['price']) #log of target variable\n",
    "\n",
    "X_train_o, X_test_o, y_train_o, y_test_o = train_test_split(X_o, y_o, test_size=0.2, random_state=42)\n",
    "print(f'Training Features shape: {X_train.shape}, Test Features Shape: {X_test_o.shape}')"
   ],
   "id": "8390ac842d0ab002",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import optuna\n",
    "#Optuna optimization\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10)\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(**params, random_state=42)\n",
    "    model.fit(X_train_o, y_train_o)\n",
    "    predictions = model.predict(X_test_o)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_o, predictions))\n",
    "    return rmse\n",
    "\n",
    "print('Hacking the mainframe...')\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "\n",
    "best_params = study.best_params\n",
    "print('Best parameters:', best_params)"
   ],
   "id": "f773daeba327aa9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "final_model = XGBRegressor(**best_params, random_state=42)\n",
    "final_model.fit(X_train_o, y_train_o)\n",
    "final_predictions = final_model.predict(X_test_o)\n",
    "\n",
    "#Model's rmse\n",
    "rmse_final = np.sqrt(mean_squared_error(y_test_o, final_predictions))\n",
    "r2_final = r2_score(y_test_o, final_predictions)\n",
    "print(f'RMSE After Optuna: {rmse_final:.2f}, R2: {r2_final:.2f}')"
   ],
   "id": "6ee8198efa034e44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Visualize results\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(np.expm1(y_test_o), np.expm1(final_predictions), alpha=0.6, color='orange')\n",
    "plt.plot([np.expm1(y_test_o).min(), np.expm1(y_test_o).max()],\n",
    "         [np.expm1(y_test_o).min(), np.expm1(y_test_o).max()], '--r')\n",
    "plt.title('Real Price vs. Predicted')\n",
    "plt.xlabel('Real Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.show()"
   ],
   "id": "f03429f3497b957",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#transform data back\n",
    "final_rmse = np.sqrt(mean_squared_error(np.expm1(y_test_o), np.expm1(final_predictions)))\n",
    "print(f'RMSE on real data: {final_rmse:.2f}')"
   ],
   "id": "6723412226610c42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "error_analysis = pd.DataFrame({'Real Price': np.expm1(y_test_o), 'Predicted Price': np.expm1(final_predictions)})\n",
    "error_analysis['Error'] = error_analysis['Real Price'] - error_analysis['Predicted Price']\n",
    "sns.histplot(error_analysis['Error'], kde=True)\n",
    "plt.title(\"Predictions Error\")\n",
    "plt.show()"
   ],
   "id": "bf2cc80e22c8ef18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Optuna using Mean absolute error\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10)\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(**params, random_state=42)\n",
    "    model.fit(X_train_o, y_train_o)\n",
    "    predictions = model.predict(X_test_o)\n",
    "    mae = mean_absolute_error(y_test_o, predictions)\n",
    "    return mae\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "final_mae = mean_absolute_error(np.expm1(y_test_o), np.expm1(final_predictions))\n",
    "print(f\"MAE after Optuna: {final_mae:.2f}\")"
   ],
   "id": "59a153923c4aff72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "errors = np.abs(np.expm1(y_test_o) - np.expm1(final_predictions))\n",
    "\n",
    "#Error histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(errors, bins=30, kde=True, color='orange')\n",
    "plt.title(\"Distribution of absolute errors\")\n",
    "plt.xlabel(\"Absolute error (price)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ],
   "id": "1250acab9317d79d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7ec025293d92f7a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "bfecb43cef85299e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "2c62f3a7ce85c04f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6e83689e5145b78d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
